# -*- coding: utf-8 -*-
"""vqa_vilt_base_v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1he32HhsLQ_95L9rnEzNUZrwm-rN6iV7p

## Download & Import
"""

!pip install transformers

!unzip /content/drive/MyDrive/VQA/open.zip

import os
import pandas as pd

import torch
import torch.optim as optim
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

from torchvision import transforms
from PIL import Image

from transformers import ViltProcessor, ViltForQuestionAnswering

from tqdm.auto import tqdm

"""## Processing Answer to Label

> 들여쓴 블록


"""

import re

contractions = {
    "aint": "ain't",
    "arent": "aren't",
    "cant": "can't",
    "couldve": "could've",
    "couldnt": "couldn't",
    "couldn'tve": "couldn't've",
    "couldnt've": "couldn't've",
    "didnt": "didn't",
    "doesnt": "doesn't",
    "dont": "don't",
    "hadnt": "hadn't",
    "hadnt've": "hadn't've",
    "hadn'tve": "hadn't've",
    "hasnt": "hasn't",
    "havent": "haven't",
    "hed": "he'd",
    "hed've": "he'd've",
    "he'dve": "he'd've",
    "hes": "he's",
    "howd": "how'd",
    "howll": "how'll",
    "hows": "how's",
    "Id've": "I'd've",
    "I'dve": "I'd've",
    "Im": "I'm",
    "Ive": "I've",
    "isnt": "isn't",
    "itd": "it'd",
    "itd've": "it'd've",
    "it'dve": "it'd've",
    "itll": "it'll",
    "let's": "let's",
    "maam": "ma'am",
    "mightnt": "mightn't",
    "mightnt've": "mightn't've",
    "mightn'tve": "mightn't've",
    "mightve": "might've",
    "mustnt": "mustn't",
    "mustve": "must've",
    "neednt": "needn't",
    "notve": "not've",
    "oclock": "o'clock",
    "oughtnt": "oughtn't",
    "ow's'at": "'ow's'at",
    "'ows'at": "'ow's'at",
    "'ow'sat": "'ow's'at",
    "shant": "shan't",
    "shed've": "she'd've",
    "she'dve": "she'd've",
    "she's": "she's",
    "shouldve": "should've",
    "shouldnt": "shouldn't",
    "shouldnt've": "shouldn't've",
    "shouldn'tve": "shouldn't've",
    "somebody'd": "somebodyd",
    "somebodyd've": "somebody'd've",
    "somebody'dve": "somebody'd've",
    "somebodyll": "somebody'll",
    "somebodys": "somebody's",
    "someoned": "someone'd",
    "someoned've": "someone'd've",
    "someone'dve": "someone'd've",
    "someonell": "someone'll",
    "someones": "someone's",
    "somethingd": "something'd",
    "somethingd've": "something'd've",
    "something'dve": "something'd've",
    "somethingll": "something'll",
    "thats": "that's",
    "thered": "there'd",
    "thered've": "there'd've",
    "there'dve": "there'd've",
    "therere": "there're",
    "theres": "there's",
    "theyd": "they'd",
    "theyd've": "they'd've",
    "they'dve": "they'd've",
    "theyll": "they'll",
    "theyre": "they're",
    "theyve": "they've",
    "twas": "'twas",
    "wasnt": "wasn't",
    "wed've": "we'd've",
    "we'dve": "we'd've",
    "weve": "we've",
    "werent": "weren't",
    "whatll": "what'll",
    "whatre": "what're",
    "whats": "what's",
    "whatve": "what've",
    "whens": "when's",
    "whered": "where'd",
    "wheres": "where's",
    "whereve": "where've",
    "whod": "who'd",
    "whod've": "who'd've",
    "who'dve": "who'd've",
    "wholl": "who'll",
    "whos": "who's",
    "whove": "who've",
    "whyll": "why'll",
    "whyre": "why're",
    "whys": "why's",
    "wont": "won't",
    "wouldve": "would've",
    "wouldnt": "wouldn't",
    "wouldnt've": "wouldn't've",
    "wouldn'tve": "wouldn't've",
    "yall": "y'all",
    "yall'll": "y'all'll",
    "y'allll": "y'all'll",
    "yall'd've": "y'all'd've",
    "y'alld've": "y'all'd've",
    "y'all'dve": "y'all'd've",
    "youd": "you'd",
    "youd've": "you'd've",
    "you'dve": "you'd've",
    "youll": "you'll",
    "youre": "you're",
    "youve": "you've",
}

manual_map = {
    "none": "0",
    "zero": "0",
    "one": "1",
    "two": "2",
    "three": "3",
    "four": "4",
    "five": "5",
    "six": "6",
    "seven": "7",
    "eight": "8",
    "nine": "9",
    "ten": "10",
}
articles = ["a", "an", "the"]
period_strip = re.compile("(?!<=\d)(\.)(?!\d)")
comma_strip = re.compile("(\d)(\,)(\d)")
punct = [
    ";",
    r"/",
    "[",
    "]",
    '"',
    "{",
    "}",
    "(",
    ")",
    "=",
    "+",
    "\\",
    "_",
    "-",
    ">",
    "<",
    "@",
    "`",
    ",",
    "?",
    "!",
]


def normalize_word(token):
    _token = token
    for p in punct:
        if (p + " " in token or " " + p in token) or (
            re.search(comma_strip, token) != None
        ):
            _token = _token.replace(p, "")
        else:
            _token = _token.replace(p, " ")
    token = period_strip.sub("", _token, re.UNICODE)

    _token = []
    temp = token.lower().split()
    for word in temp:
        word = manual_map.setdefault(word, word)
        if word not in articles:
            _token.append(word)
    for i, word in enumerate(_token):
        if word in contractions:
            _token[i] = contractions[word]
    token = " ".join(_token)
    token = token.replace(",", "")
    return token

import pandas as pd

df = pd.read_csv('train.csv')

from collections import Counter

# The original function to assign scores based on the occurences
def get_score(occurences):
    if occurences == 0:
        return 0.0
    elif occurences < 3:
        return 0.3
    elif occurences < 6:
        return 0.6
    elif occurences < 9:
        return 0.9
    else:
        return 1.0

# Apply the normalize_word function to the 'answer' column
df['answer'] = df['answer'].apply(normalize_word)

# Count the occurences of each answer
answer_counter = Counter(df['answer'])

# Create a new column for the occurences
df['answer_occurences'] = df['answer'].apply(lambda x: answer_counter[x])

# Create a new column for the scores
df['answer_scores'] = df['answer_occurences'].apply(get_score)

# Create a new column for the labels
# The labels are created by enumerating the unique answers
unique_answers = df['answer'].unique()
answer2label = {k: i for i, k in enumerate(unique_answers)}
label2answer = list(answer_counter.keys())
df['answer_labels'] = df['answer'].apply(lambda x: answer2label[x])

df.to_csv('train_updated.csv', index=False)

import pickle

# Save the answer2label and label2answer mappings
with open('answer2label.pkl', 'wb') as f:
    pickle.dump(answer2label, f)

with open('label2answer.pkl', 'wb') as f:
    pickle.dump(label2answer, f)

import pickle

with open('answer2label.pkl', 'rb') as f:
    answer2label = pickle.load(f)

with open('label2answer.pkl', 'rb') as f:
    label2answer = pickle.load(f)

"""## Data Set"""

import random

import PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw
import numpy as np
import torch
from PIL import Image


def ShearX(img, v):  # [-0.3, 0.3]
    assert -0.3 <= v <= 0.3
    if random.random() > 0.5:
        v = -v
    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))


def ShearY(img, v):  # [-0.3, 0.3]
    assert -0.3 <= v <= 0.3
    if random.random() > 0.5:
        v = -v
    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))


def TranslateX(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]
    assert -0.45 <= v <= 0.45
    if random.random() > 0.5:
        v = -v
    v = v * img.size[0]
    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))


def TranslateXabs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]
    assert 0 <= v
    if random.random() > 0.5:
        v = -v
    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))


def TranslateY(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]
    assert -0.45 <= v <= 0.45
    if random.random() > 0.5:
        v = -v
    v = v * img.size[1]
    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))


def TranslateYabs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]
    assert 0 <= v
    if random.random() > 0.5:
        v = -v
    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))


def Rotate(img, v):  # [-30, 30]
    assert -30 <= v <= 30
    if random.random() > 0.5:
        v = -v
    return img.rotate(v)


def AutoContrast(img, _):
    return PIL.ImageOps.autocontrast(img)


def Invert(img, _):
    return PIL.ImageOps.invert(img)


def Equalize(img, _):
    return PIL.ImageOps.equalize(img)


def Flip(img, _):  # not from the paper
    return PIL.ImageOps.mirror(img)


def Solarize(img, v):  # [0, 256]
    assert 0 <= v <= 256
    return PIL.ImageOps.solarize(img, v)


def SolarizeAdd(img, addition=0, threshold=128):
    img_np = np.array(img).astype(np.int)
    img_np = img_np + addition
    img_np = np.clip(img_np, 0, 255)
    img_np = img_np.astype(np.uint8)
    img = Image.fromarray(img_np)
    return PIL.ImageOps.solarize(img, threshold)


def Posterize(img, v):  # [4, 8]
    v = int(v)
    v = max(1, v)
    return PIL.ImageOps.posterize(img, v)


def Contrast(img, v):  # [0.1,1.9]
    assert 0.1 <= v <= 1.9
    return PIL.ImageEnhance.Contrast(img).enhance(v)


def Color(img, v):  # [0.1,1.9]
    assert 0.1 <= v <= 1.9
    return PIL.ImageEnhance.Color(img).enhance(v)


def Brightness(img, v):  # [0.1,1.9]
    assert 0.1 <= v <= 1.9
    return PIL.ImageEnhance.Brightness(img).enhance(v)


def Sharpness(img, v):  # [0.1,1.9]
    assert 0.1 <= v <= 1.9
    return PIL.ImageEnhance.Sharpness(img).enhance(v)


def Cutout(img, v):  # [0, 60] => percentage: [0, 0.2]
    assert 0.0 <= v <= 0.2
    if v <= 0.0:
        return img

    v = v * img.size[0]
    return CutoutAbs(img, v)


def CutoutAbs(img, v):  # [0, 60] => percentage: [0, 0.2]
    # assert 0 <= v <= 20
    if v < 0:
        return img
    w, h = img.size
    x0 = np.random.uniform(w)
    y0 = np.random.uniform(h)

    x0 = int(max(0, x0 - v / 2.0))
    y0 = int(max(0, y0 - v / 2.0))
    x1 = min(w, x0 + v)
    y1 = min(h, y0 + v)

    xy = (x0, y0, x1, y1)
    color = (125, 123, 114)
    # color = (0, 0, 0)
    img = img.copy()
    PIL.ImageDraw.Draw(img).rectangle(xy, color)
    return img


def SamplePairing(imgs):  # [0, 0.4]
    def f(img1, v):
        i = np.random.choice(len(imgs))
        img2 = PIL.Image.fromarray(imgs[i])
        return PIL.Image.blend(img1, img2, v)

    return f


def Identity(img, v):
    return img


def augment_list():  # 16 oeprations and their ranges
    # https://github.com/google-research/uda/blob/master/image/randaugment/policies.py#L57
    # l = [
    #     (Identity, 0., 1.0),
    #     (ShearX, 0., 0.3),  # 0
    #     (ShearY, 0., 0.3),  # 1
    #     (TranslateX, 0., 0.33),  # 2
    #     (TranslateY, 0., 0.33),  # 3
    #     (Rotate, 0, 30),  # 4
    #     (AutoContrast, 0, 1),  # 5
    #     (Invert, 0, 1),  # 6
    #     (Equalize, 0, 1),  # 7
    #     (Solarize, 0, 110),  # 8
    #     (Posterize, 4, 8),  # 9
    #     # (Contrast, 0.1, 1.9),  # 10
    #     (Color, 0.1, 1.9),  # 11
    #     (Brightness, 0.1, 1.9),  # 12
    #     (Sharpness, 0.1, 1.9),  # 13
    #     # (Cutout, 0, 0.2),  # 14
    #     # (SamplePairing(imgs), 0, 0.4),  # 15
    # ]

    # https://github.com/tensorflow/tpu/blob/8462d083dd89489a79e3200bcc8d4063bf362186/models/official/efficientnet/autoaugment.py#L505
    l = [
        (AutoContrast, 0, 1),
        (Equalize, 0, 1),
        # (Invert, 0, 1),
        (Rotate, 0, 30),
        (Posterize, 0, 4),
        (Solarize, 0, 256),
        (SolarizeAdd, 0, 110),
        (Color, 0.1, 1.9),
        (Contrast, 0.1, 1.9),
        (Brightness, 0.1, 1.9),
        (Sharpness, 0.1, 1.9),
        (ShearX, 0.0, 0.3),
        (ShearY, 0.0, 0.3),
        # (CutoutAbs, 0, 40),
        (TranslateXabs, 0.0, 100),
        (TranslateYabs, 0.0, 100),
    ]

    return l


class Lighting(object):
    """Lighting noise(AlexNet - style PCA - based noise)"""

    def __init__(self, alphastd, eigval, eigvec):
        self.alphastd = alphastd
        self.eigval = torch.Tensor(eigval)
        self.eigvec = torch.Tensor(eigvec)

    def __call__(self, img):
        if self.alphastd == 0:
            return img

        alpha = img.new().resize_(3).normal_(0, self.alphastd)
        rgb = (
            self.eigvec.type_as(img)
            .clone()
            .mul(alpha.view(1, 3).expand(3, 3))
            .mul(self.eigval.view(1, 3).expand(3, 3))
            .sum(1)
            .squeeze()
        )

        return img.add(rgb.view(3, 1, 1).expand_as(img))


class CutoutDefault(object):
    """
    Reference : https://github.com/quark0/darts/blob/master/cnn/utils.py
    """

    def __init__(self, length):
        self.length = length

    def __call__(self, img):
        h, w = img.size(1), img.size(2)
        mask = np.ones((h, w), np.float32)
        y = np.random.randint(h)
        x = np.random.randint(w)

        y1 = np.clip(y - self.length // 2, 0, h)
        y2 = np.clip(y + self.length // 2, 0, h)
        x1 = np.clip(x - self.length // 2, 0, w)
        x2 = np.clip(x + self.length // 2, 0, w)

        mask[y1:y2, x1:x2] = 0.0
        mask = torch.from_numpy(mask)
        mask = mask.expand_as(img)
        img *= mask
        return img


class RandAugment:
    def __init__(self, n, m):
        self.n = n
        self.m = m  # [0, 30]
        self.augment_list = augment_list()

    def __call__(self, img):
        ops = random.choices(self.augment_list, k=self.n)
        for op, minval, maxval in ops:
            val = (float(self.m) / 30) * float(maxval - minval) + minval
            img = op(img, val)

        return img

class VQADataset(Dataset):
    def __init__(self, df, processor, img_path, is_test=False):
        self.df = df
        self.processor = processor
        self.img_path = img_path
        self.is_test = is_test

        self.randaugment = RandAugment(2, 9)  # You can adjust the parameters as needed

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]

        img_name = os.path.join(self.img_path, row['image_id'] + '.jpg')
        image = PIL.Image.open(img_name).convert('RGB')

        # Apply RandAugment if not in test mode
        if not self.is_test:
            image = self.randaugment(image)

        question = row['question']
        question = question.replace('?', ' ? ')
        question = question.replace('.', ' . ')
        question = question.replace(',', ' . ')
        question = question.replace('!', ' . ')

        max_length = 40
        encoding = self.processor(image, question, padding="max_length", return_tensors="pt")
        for k,v in encoding.items():
          encoding[k] = v.squeeze()

        if not self.is_test:
            answer = row['answer']
            label = row['answer_labels']
            score = row['answer_scores']
            targets = torch.zeros(len(answer2label))
            targets[label] = score
            encoding['labels'] = targets

        else :
            targets = torch.zeros(len(answer2label))
            encoding['labels'] = targets

        return encoding

def collate_fn(batch):
  input_ids = [item['input_ids'] for item in batch]
  pixel_values = [item['pixel_values'] for item in batch]
  attention_mask = [item['attention_mask'] for item in batch]
  token_type_ids = [item['token_type_ids'] for item in batch]
  labels = [item['labels'] for item in batch]

  # create padded pixel values and corresponding pixel mask
  encoding = processor.image_processor.pad(pixel_values, return_tensors="pt")

  # create new batch
  batch = {}
  batch['input_ids'] = torch.stack(input_ids)
  batch['attention_mask'] = torch.stack(attention_mask)
  batch['token_type_ids'] = torch.stack(token_type_ids)
  batch['pixel_values'] = encoding['pixel_values']
  batch['pixel_mask'] = encoding['pixel_mask']
  batch['labels'] = torch.stack(labels)

  return batch

train_df = pd.read_csv('train_updated.csv')
test_df = pd.read_csv('test.csv')
sample_submission = pd.read_csv('sample_submission.csv')
train_img_path = 'image/train'
test_img_path = 'image/test'

processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-mlm")

train_dataset = VQADataset(train_df, processor, train_img_path, is_test=False)
train_loader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=32, shuffle=True)

batch = next(iter(train_loader))

for k,v in batch.items():
  print(k, v.shape)

"""## Train"""

# 모델을 학습하는 함수
def train(model, train_loader, optimizer):
    # 모델을 학습 모드로 설정
    model.train()
    # 전체 손실을 초기화
    total_loss = 0

    # DataLoader에서 mini batch 단위로 데이터를 순회
    for batch in tqdm(train_loader):
        # 데이터를 GPU로 전송
        batch = {k: v.to(device) for k, v in batch.items()}

        # 옵티마이저의 변화도 초기화
        optimizer.zero_grad()

        # 모델에 배치 데이터를 전달하고, 예측 값(Outputs)을 반환
        outputs = model(**batch)
        loss = outputs.loss

        total_loss += loss.item()
        print("Loss:", loss.item())
        loss.backward()
        optimizer.step()

    avg_loss = total_loss / len(train_loader)
    return avg_loss

# 모델의 추론을 수행하는 함수
def inference(model, loader):
    # 모델을 평가 모드로 설정
    model.eval()
    # 예측 결과를 저장할 빈 리스트 생성
    preds = []

    # 추론 과정에서 gradient를 계산하지 않도록 설정
    # DataLoader에서 mini batch 단위로 데이터를 순회
    with torch.no_grad():
        for data in tqdm(loader, total=len(loader)):
            # 데이터를 GPU로 전송
            data = {k: v.to(device) for k, v in data.items()}

            # 이미지와 질문 데이터를 모델에 전달하고, 예측값(outputs)을 반환
            outputs = model(**data)

            logits = outputs.logits
            predicted_class = logits.argmax(-1)
            pred = [label2answer[pred_class.item()] for pred_class in predicted_class]

            preds.extend(pred)

    return preds

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
print(f"current device is {device}")

model = ViltForQuestionAnswering.from_pretrained("dandelin/vilt-b32-mlm",
                                                 hidden_dropout_prob = 0.1,
                                                 attention_probs_dropout_prob = 0.1,
                                                 num_labels=len(answer2label),
                                                 id2label=answer2label,
                                                 label2id=label2answer)

model.to(device)

max_epoch = 5
max_steps = None
optim_type = "adamw"
learning_rate = 3e-5
weight_decay = 0.05

# Create an optimizer
if optim_type == "adamw":
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

!pip install accelerate

from accelerate import Accelerator

accelerator = Accelerator()
device = accelerator.device

model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)

save_dir = "/content/drive/MyDrive/saved_models_vilt"
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

for epoch in range(1):  # 이어서 학습을 2 에폭 더 진행하도록 수정
    avg_loss = train(model, train_loader, optimizer)
    print(f"Epoch: {epoch}, Loss: {avg_loss:.4f}")

    # 1 에폭 학습이 완료될 때마다 모델 가중치를 저장
    save_path = os.path.join(save_dir, f"model_epoch_6.pth")
    torch.save(model.state_dict(), save_path)

print("Training finished.")

optim_type = "adamw"
learning_rate = 1e-5
weight_decay = 0.05

# Create an optimizer
if optim_type == "adamw":
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

save_dir = "/content/drive/MyDrive/saved_models_vilt"
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

for epoch in range(1):  # 이어서 학습을 2 에폭 더 진행하도록 수정
    avg_loss = train(model, train_loader, optimizer)
    print(f"Epoch: {epoch}, Loss: {avg_loss:.4f}")

    # 1 에폭 학습이 완료될 때마다 모델 가중치를 저장
    save_path = os.path.join(save_dir, f"model_epoch_7.pth")
    torch.save(model.state_dict(), save_path)

print("Training finished.")

# Dataset & DataLoader
test_dataset = VQADataset(test_df, processor, test_img_path, is_test=True)
test_loader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=32, shuffle=False)

# inference
preds = inference(model, test_loader)

sample_submission['answer'] = preds
sample_submission.to_csv('/content/drive/MyDrive/submission_7.csv', index=False)

# 기존에 학습한 모델의 가중치를 불러옴
checkpoint_path = "/content/drive/MyDrive/saved_models/model_epoch_6.pth"
if os.path.exists(checkpoint_path):
    model.load_state_dict(torch.load(checkpoint_path))
    print("Loaded model weights from", checkpoint_path)
else:
    print("No checkpoint found at", checkpoint_path)
    exit(1)

import IPython
from tensorflow.keras import backend as K

def clear_ram():
    IPython.Application.instance().kernel.do_shutdown(True)

def clear_gpu_ram():
    K.clear_session()

clear_ram()
clear_gpu_ram()

"""## Experiments"""

# CSV 파일을 읽어서 상위 5개의 행을 확인합니다.
import pandas as pd

file_path = "train.csv"
train_data = pd.read_csv(file_path)

# 파일을 다시 읽고 'answer' 열의 빈도수를 계산합니다.
df = pd.read_csv(file_path)
answer_counts = df['answer'].value_counts()

# 빈도수에 따라 label의 개수를 분류합니다.
frequency_bins = {
    "1-2": 0,
    "3-5": 0,
    "6-8": 0,
    "9+": 0
}

for count in answer_counts:
    if count >= 1 and count <= 2:
        frequency_bins["1-2"] += 1
    elif count >= 3 and count <= 5:
        frequency_bins["3-5"] += 1
    elif count >= 6 and count <= 8:
        frequency_bins["6-8"] += 1
    else:
        frequency_bins["9+"] += 1

# 비율로 표시
total_labels = sum(frequency_bins.values())
frequency_ratios = {key: value / total_labels * 100 for key, value in frequency_bins.items()}
frequency_ratios

import matplotlib.pyplot as plt

question_lengths = train_data['question'].apply(lambda x: len(x.split()))

# 질문 길이의 히스토그램을 그립니다.
plt.hist(question_lengths, bins=range(1, question_lengths.max() + 1), edgecolor='black')
plt.title('Distribution of Question Lengths')
plt.xlabel('Length of Question (words)')
plt.ylabel('Frequency')
plt.show()

question_lengths.describe()

answer_lengths = train_data['answer'].apply(lambda x: len(x.split()))

# 답변 길이의 히스토그램을 그립니다.
plt.hist(answer_lengths, bins=range(1, answer_lengths.max() + 1), edgecolor='black')
plt.title('Distribution of Answer Lengths')
plt.xlabel('Length of Answer (words)')
plt.ylabel('Frequency')
plt.show()

answer_distribution = train_data['answer'].value_counts()

# 답변의 유형을 분석하기 위해 분류합니다.
short_answers = ['yes', 'no']
numeric_answers = answer_distribution.index[answer_distribution.index.str.isnumeric()]

def categorize_answer(answer):
    if answer in short_answers:
        return "Short Answer (Yes/No)"
    elif answer in numeric_answers:
        return "Numeric Answer"
    else:
        return "Other"

# 각 답변의 카테고리를 계산하고 분포를 확인합니다.
answer_categories = train_data['answer'].apply(categorize_answer)
answer_category_distribution = answer_categories.value_counts(normalize=True) * 100
answer_category_distribution

def most_representative_word(words):
    min_distance = float('inf')
    representative_word = None
    for word1 in words:
        total_distance = 0
        for word2 in words:
            if word1 != word2:
                total_distance += distance(word1, word2)
        if total_distance < min_distance:
            min_distance = total_distance
            representative_word = word1
    return representative_word

def create_vocab():
    # CSV 파일에서 데이터를 읽어옵니다.
    file_path = "train.csv"
    df = pd.read_csv(file_path)

    # 답변에서 가장 흔한 답변을 선택하고 대표 단어를 찾습니다.
    for i in range(len(df)):
        # 질문에 대한 답변을 가져옵니다.
        answers_for_q = [df["answer"].iloc[i]]

        # 여기서는 답변이 하나만 있으므로, 그것이 가장 흔한 답변이 됩니다.
        most_common_answer = answers_for_q[0]

        # 가장 흔한 답변을 설정합니다.
        df['answer'].iloc[i] = most_common_answer

    # 열 이름을 'class_name'으로 변경합니다.
    df = df.rename(columns={"answer": "class_name"})

    # 빈번한 답변별로 정렬합니다.
    counts = pd.DataFrame(df["class_name"].value_counts())

    # 클래스 인덱스를 추가합니다.
    counts["class_index"] = range(len(counts))

    # 클래스 매핑 데이터프레임을 생성합니다.
    class_mapping = pd.DataFrame(
        {"answer": counts.index, "class_id": range(len(counts))}
    )

    # 빈 키와 class_mapping의 길이를 값으로 추가합니다.
    class_mapping = class_mapping.append(
        {"answer": "", "class_id": len(class_mapping)}, ignore_index=True
    )

    # 클래스 매핑 데이터프레임을 저장합니다.
    class_mapping_path = "class_mapping.csv"
    class_mapping.to_csv(class_mapping_path, index=False)

    return class_mapping

# 함수를 실행하여 클래스 매핑을 생성합니다.
class_mapping_result = create_vocab()
class_mapping_result.head()